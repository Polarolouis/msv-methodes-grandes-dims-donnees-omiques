---
title: "Présentation de projet : Méthodes de statistiques en grande dimension pour l'analyse de données de biologie moléculaire"
author: Louis Lacoste
date: 13 Février, 2024
output: 
  beamer_presentation:
    theme: Berlin
    slide_level: 2
bibliography: references.bib
header-includes:
  - \setbeamertemplate{page number in head/foot}[totalframenumber]
  - \usepackage{bm}
  - \DeclareMathOperator{\ind}{\perp \!\!\! \perp}
---

```{r useful, echo = FALSE, include = FALSE}
require(MultiVarSel)
require(ggplot2)
require(tidyr)
require(FactoMineR)
require(gridExtra)
```

```{r import_donnees, echo = FALSE}
# Import données
table <- read.table("Table_metabolome_FH_all.csv", header = TRUE, sep = ";", dec = ",")

# Retrait des 4 premières colonnes
table2 <- table[, -seq(1, 4)]

table2$temperature <- as.factor(table2$temperature)
table2$imbibition <- as.factor(table2$imbibition)
```

# Présentation des données

## Contexte biologique

Le jeu de données choisi étudie l'influence de 2 paramètres sur la 
**capacité germinative** des graines chez \emph{Arabidopsis Thaliana} à l'aide de données de métabolomique
pour des graines fraîchement récoltées (***F****reshly* ***H****arvested*).

## Les paramètres considérés

### La température, variable qualitative à 3 niveaux :
- *Low*
- *Medium*
- *Elevated*

### Le stade d'imbibition, variable qualitative à 3 modalités : 
- DS (*Dry seed*)
- EI après 6h d’imbibition (*Early imbibition*) correspondant à la fin de la prise d’eau, 
- LI après 20h d’imbibition (*Late imbibition*) 


## Extrait des données

Nous présentons ici les 7 premières colonnes (7 sur 237) du jeu de données\footnote{Les 4 
premières colonnes présentent différentes informations pour identifier 
les conditions expérimentales et la répétition.} : 

\begin{columns}
\begin{column}{0.3\textwidth}
\tiny
```{r, echo = FALSE}
knitr::kable(table[1:6,5:6], format = "latex", booktabs = TRUE)
```
\end{column}
\begin{column}{0.65\textwidth}
\tiny
```{r, echo = FALSE}
knitr::kable(table[1:6,7:11], format = "latex", booktabs = TRUE)
```
\end{column}
\end{columns}


# Statistiques descriptives et classification

### Vérifications élémentaires
```{r verification_elementaire, echo = FALSE}
# Moyennes des colonnes
idx_null_mean <- as.integer(length(which(colMeans(table2[, 3:dim(table2)[2]]) == 0)))

# Pour retirer les metabolomes sans variations
idx_null_sd <- as.integer(length(which(apply(table2[, 3:dim(table2)[2]], 2, sd) == 0)))

# Verifier les NAs
idx_NAs <- as.integer(length(which(is.na(table2))))

knitr::asis_output(paste(
    "Il y a", idx_null_mean, "colonnes dont la moyenne est nulle,", idx_null_sd,
    "colonnes dont l'écart-type est nul et", idx_NAs, "NAs."
))
```

## ACP

```{r acp, echo=FALSE}
# TODO Faire des stats descriptives moyennes, ACP, clustering ...
res.pca = PCA(table2, scale.unit=TRUE, ncp=5, quali.sup = c(1,2), graph = FALSE)
plot13_pca_var <- plot.PCA(res.pca, axes=c(1, 3), choix="var", habillage="cos2")
plot23_pca_var <- plot.PCA(res.pca, axes=c(2, 3), choix="var", habillage="cos2")
grid.arrange(plot13_pca_var, plot23_pca_var, ncol = 2)
```

---

```{r, echo=FALSE}
plot13_pca_ind <- plot.PCA(res.pca, axes=c(1, 3), choix="ind", habillage=1)
plot23_pca_ind <- plot.PCA(res.pca, axes=c(2, 3), choix="ind", habillage=2)
grid.arrange(plot13_pca_ind, plot23_pca_ind, ncol = 2)
```

---

```{r , echo = FALSE, fig.cap="Pourcentage de variance expliqué par les composantes"}
barplot(res.pca$eig[,1]/sum(res.pca$eig[,1]), ylim = c(0,1))
```

## Clustering (*K-means*)

```{r kmeans, echo = FALSE, include=FALSE}
require(factoextra)
library(cluster)
```
```{r kmeans_optim, echo=FALSE, fig.height=6}
data_cluster <- scale(table2[,-c(1,2)])

fviz_nbclust(data_cluster, kmeans, method = "wss")
```

## $K = 4$

```{r kmeans_4, echo = FALSE, fig.height=6}
set.seed(1234)
km <- kmeans(data_cluster, centers = 4, nstart = 25)
fviz_cluster(km, data = data_cluster) + ggtitle("K = 4")
```

---

Nous avons tenté de faire des moyennes par groupe sans voir de tendance se 
dégager clairement.

```{r kmeans_4_mean, echo = FALSE}
# aggregate(table2[,-c(1,2)], by=list(cluster=km$cluster), mean)
```
\begin{columns}
\begin{column}{0.45\textwidth}
\tiny
```{r table_km4_1, echo = FALSE}
knitr::kable(cbind(km$cluster,table2[c(1,2)])[1:13,], format = "latex", booktabs = TRUE)
```
\end{column}
\begin{column}{0.45\textwidth}
\tiny
```{r table_km4_2, echo = FALSE}
knitr::kable(cbind(km$cluster,table2[c(1,2)])[14:27,], format = "latex", booktabs = TRUE)
```
\end{column}
\end{columns}


## $K = 9$

```{r kmeans_9, echo = FALSE, fig.height=6}
set.seed(1234)
km9 <- kmeans(data_cluster, centers = 9, nstart = 25)
fviz_cluster(km9, data = data_cluster) + ggtitle("K = 9")
```

---

\begin{columns}
\begin{column}{0.45\textwidth}
\tiny
```{r table_km9_1, echo = FALSE}
knitr::kable(cbind(km9$cluster,table2[c(1,2)])[1:13,], format = "latex", booktabs = TRUE)
```
\end{column}
\begin{column}{0.45\textwidth}
\tiny
```{r table_km9_2, echo = FALSE}
knitr::kable(cbind(km9$cluster,table2[c(1,2)])[14:27,], format = "latex", booktabs = TRUE)
```
\end{column}
\end{columns}
# Méthode

## Principe du modèle

Nous allons poser le modèle suivant (détaillé sur les slides suivantes):
$$\overbrace{\bm{Y}}^{\text{matrice des observations}} = \bm{\underbrace{X}_{\text{matrice de design}}B} + 
\underbrace{\bm{E}}_{\text{erreur résiduelle}}$$
Nous supposons que :
$$\forall i \in \{1, \dots n\}, (E_{i,1}, \dots, E_{i,q}) \sim \mathcal{N}(0, \bm \Sigma)$$
et que $\forall (i,k) \in \{1,\dots,n\}^2  | i \neq k, (E_{i,1}, \dots, E_{i,q}) \ind (E_{k,1}, \dots, E_{k,q})$

Et nous cherchons à estimer $\bm B$, la matrice des coefficients, par $\widehat{\bm{B}}$ en faisant en sorte que 
l'estimateur soit parcimonieux car $\bm B$ est supposée contenir beaucoup de 
zéros.

## Modèle linéaire général, ANOVA interaction à 2 facteurs

```{r X_Y, echo = FALSE}
Y <- as.matrix(table2[,-c(1,2)])

# Permet de récupérer la matrice de design
X <- model.matrix(lm(Y~temperature:imbibition + 0, data=table2))

p <- ncol(X)
n <- nrow(X)

q <- dim(Y)[2]

## Rescale de Y pour éviter de donner de l'importance a certine variable
Y <- scale(Y)
```

$$\bm{Y} = \bm{XB} + \bm{E}$$

\begin{columns}
  \begin{column}{0.6\textwidth}
      $$\bm{X} = \left. 
        \mathop{\begin{pmatrix}
          \bm{1}_{Ele.:DS} & \dots  & \bm{1}_{Med.:LI}\\
        \end{pmatrix}}_{p = `r p`}
      \right.{\scriptstyle n = `r n`} $$

      \tiny Avec $\bm{1}_{A:B, i} = 1$ ou $0$ si la $i^e$ observation n'est pas 
      dans les 2 modalités A et B.

  \end{column}
  \begin{column}{0.4\textwidth}
      $$\bm{B} = \left.  
      \mathop{
        \begin{pmatrix}
          B_{1,1} & \cdots  & B_{1,q}\\
          \vdots & \cdots  & \vdots \\
          B_{p,1} & \cdots  & B_{p,q}\\
        \end{pmatrix}
      }_{q = `r q`} \right. {\scriptstyle p = `r p`} $$
  \end{column}
\end{columns}

\small Les observations pour les différentes valeurs de métabolites sont alors mise en
forme dans la matrice $\bm{Y}$ :

$\bm{Y} = \left.
\mathop{\begin{pmatrix}
Y_{1,1} & \cdots & Y_{1,q}\\
\vdots & \cdots & \vdots \\
Y_{n,1} & \cdots & Y_{n,q}\\
\end{pmatrix}}_{q = `r q`} \right. {\scriptstyle n = `r n`}$ Y a été centrée et réduite.

## Calcul des résidus

Ici nous allons ajuster le modèle linéaire en faisant comme si les colonnes de 
$\bm Y$ étaient indépendantes afin d'estimer par $\bm{\widehat{E}}$ la matrice
$\bm E$, l'erreur résiduelle.

Puis nous allons tester avec le test de Portmanteau 
(grâce au théorème de Bartlett) si chaque ligne de $\bm{\widehat{E}}$ est un 
bruit blanc.

```{r residus, echo = FALSE}
# Résidus
residus <- lm(as.matrix(Y) ~ X - 1)$residuals

pvalue <- whitening_test(residus)

# La pvaleur est petit par rapport à un seuil 10% donc les données ne sont pas blanche
```

En calculant les résidus du modèle linéaire on obtient une *p-value* de 
$`r round(pvalue, digits = 3)`$ qui est à peine au-dessus du seuil 5%.
Malgré tout nous allons voir si le blanchiment permettrait d'améliorer cela.

## Principe du blanchiment

Le principe du \emph{blanchiment} est de \textbf{supprimer les corrélations existant entre 
les colonnes.}

Pour cela il faut estimer $\bm \Sigma^{-1/2}$ et alors le modèle se ré-écrit :
$$\bm Y \bm \Sigma^{-1/2} = \bm X \bm B \bm \Sigma^{-1/2} + \bm E \bm \Sigma^{-1/2}$$

Puis on peut appliquer le critère LASSO et la *stability selection* sur le 
modèle vectorisé :
$$\mathcal Y = \mathcal X \mathcal B + \mathcal E$$
avec
$$\mathcal Y = vec(\bm Y \bm \Sigma^{-1/2}), \mathcal X = (\bm \Sigma^{-1/2}) \otimes \bm X, \mathcal B = vec(\bm B), \mathcal E = vec(\bm E \bm \Sigma^{-1/2})$$

---

Il faut donc estimer $\bm \Sigma^{-1/2}$ avec un estimateur $\widehat{\bm \Sigma}^{-1/2}$.

Pour cela le package R `MultiVarSel` (@perrot-dockesIntroductionMultiVarSel2019) 
permet d'utiliser 3 structures de dépendances et implémente les méthodes 
d'estimation de $\widehat{\Sigma}^{-1/2}$ pour chacun des cas :

- $AR(1)$
- $ARMA(p,q)$
- Non paramétrique\footnote{Suppose uniquement que le processus est 
stationnaire.}

## Blanchiment des données

Et ainsi la méthode qui blanchit le mieux ces données est la méthode *non 
paramétrique*\footnote{Ce qui est empiriquement régulièrement le cas.}.
Nous récupérons à la fin de cette étape la matrice $\hat{\Sigma}^{-\frac{1}{2}}$ 
permettant de blanchir les données.

```{r, echo = FALSE}
## Testons des dépendances
results_whithening_choice <- whitening_choice(residus, typeDeps = c("AR1", "nonparam", "ARMA", "no_whitening"), pAR = 1, qMA = 1)

knitr::kable(results_whithening_choice,
  caption = "Tableau de résultats des tests de Portmanteau pour les différentes méthodes"
)
## Ainsi on utilise le non param

square_root_inv_hat_Sigma=whitening(residus, "nonparam", pAR = 1, qMA = 0)
```

# Test de $\Sigma = Id$

## Principe de l'article de @fisherTestingIdentityCovariance2012

Dans cet article, Fisher développe de nouvelles statistiques de test afin de 
vérifier si l'on peut rejeter ou non l'hypothèse 
$$(H_0) : \Sigma = Id$$ 
pour les cas où $(n,q) \rightarrow +\infty$.

En utilisant les moyennes arithmétiques et leurs estimateurs 
($\hat{a_i}$)\footnote{Voir l'article pour les formules}, et $c = q/n$ 
Fisher démontre que sous $H_0$ et d'autres conditions:
\begin{align*}
T_1 & = \frac{n}{c\sqrt{8}} (\hat{a_4} - 4 \hat{a_3} + 6 \hat{a_2} - 4 \hat{a_1} + 1) \xrightarrow{D} \mathcal{N}(0,1)\\
T_2 & = \frac{n}{\sqrt{8(c^2 + 12 c + 8)}} (\hat{a_4} - 2 \hat{a_2} + 1) \xrightarrow{D} \mathcal{N}(0,1)\\
\end{align*}


## Comportement des statistiques $T_1$ et $T_2$ sur données simulées

Nous avons testé en simulant plusieurs jeux de données avec différentes 
corrélations :

- Sous un AR(1) ($\phi_1 = 0.5$), donc avec des corrélations entre colonnes.
- Les données d'AR(1) mais blanchient par $\Sigma^{-1/2}$, donc sans 
corrélations.
- Des vecteurs gaussiens donc sans corrélations.

Le tout pour différentes valeurs de $n$ répétitions indépendantes, avec des 
vecteurs de longeur $q$\footnote{correspondant au $p$ de l'article} en utilisant
le paramètre d'échelle $c$ qui donne la relation $q = c n$.

## Résultats de simulations

```{r import_donnees_simu, echo = FALSE}
N <- 50
phi1 <- 0.5
n_values <- seq(20, 300, by = 80)
c_values <- seq(1, 12)

filename <- paste0(
    "covvar_test_n_",
    paste0(n_values, collapse = "_"),
    "_c_",
    paste0(c_values, collapse = "_"),
    "_N_", N,
    "_AR_1_phi1_", phi1, ".Rds"
)

load(filename)


filename_gauss <- paste0(
    "covvar_test_n_",
    paste0(n_values, collapse = "_"),
    "_c_",
    paste0(c_values, collapse = "_"),
    "_N_", N,
    "_gaussian.Rds"
)

load(filename_gauss)


# Omit NAs
result <- result_AR1_T1_T2[complete.cases(result_AR1_T1_T2), ]

result$c <- as.numeric(result$c)
result$p <- as.numeric(result$p)
result$n <- as.numeric(result$n)
result$type <- as.factor(result$type)

result_gauss$type <- as.factor(result_gauss$type)

result <- rbind(result, result_gauss)
```

```{r graphique_simu_T1, echo=FALSE, fig.height=4.5, fig.cap="\\emph{p-value} pour le test basé sur la statistique $T_1$"}
ggplot(result) +
    aes(x = c, y = T1_pvalue_mean, fill = as.factor(n)) +
    geom_bar(stat = "identity") +
    geom_errorbar(aes(
        ymin = T1_pvalue_mean - T1_pvalue_sd,
        ymax = T1_pvalue_mean + T1_pvalue_sd
    ))+
    geom_hline(aes(yintercept = 0.05, color = "red")) +
    geom_vline(aes(xintercept = 9)) +
    scale_x_continuous(breaks = seq(1,12,1)) + 
    labs(color = "Seuil = 0.05", fill = "n") +
    facet_grid(vars(result$n), vars(result$type)) +
    ggtitle(paste0("Test sur la statistique T1 | N = ", N))
```

La ligne verticale, indique la valeur de $c$ la plus proche de nos données.

---

```{r graphique_simu_T2, echo = FALSE, fig.height=4.5, fig.cap="\\emph{p-value} pour le test basé sur la statistique $T_2$"}
ggplot(result) +
    aes(x = c, y = T2_pvalue_mean, fill = as.factor(n)) +
    geom_bar(stat = "identity") +
    geom_errorbar(aes(
        ymin = T2_pvalue_mean - T2_pvalue_sd,
        ymax = T2_pvalue_mean + T2_pvalue_sd
    ))+
    geom_hline(aes(yintercept = 0.05, color = "red")) +
    geom_vline(aes(xintercept = 9)) +
    scale_x_continuous(breaks = seq(1,12,1)) + 
    labs(color = "Seuil = 0.05", fill = "n") +
    facet_grid(vars(result$n), vars(result$type)) +
    ggtitle(paste0("Test sur la statistique T2 | N = ", N))
```

La ligne verticale, indique la valeur de $c$ la plus proche de nos données.

## Notre cas

Nous avons $n = `r n`$ et $q = `r q`$ soit $c \simeq `r round(q/n, digits = 0)`$
```{r fisher_test_our_data, echo = FALSE}
source("test_covvar_identity.R")
```

En appliquant les tests à nos données nous avons :

- Pour les données non blanchies, $\text{p-value}^{\text{avant blanch.}}_{T_1} = `r T1_pvalue(residus)`$
et $\text{p-value}^{\text{avant blanch.}}_{T_2} = `r T2_pvalue(residus)`$, ainsi le test indique que nos 
données sont corrélées, en opposition au test de Portmanteau ("no_whitening").
- Pour les données blanchies, $\text{p-value}^{\text{après blanch.}}_{T_1} = `r T1_pvalue(residus %*% square_root_inv_hat_Sigma)`$
et $\text{p-value}^{\text{après blanch.}}_{T_2} = `r T2_pvalue(residus %*% square_root_inv_hat_Sigma)`$, ainsi le test indique que nos 
données sont corrélées, en opposition au test de Portmanteau et au processus de blanchiment.

Aux vues des simulations, notre $c$ est sûrement trop grand pour ces tests.

# Résultats de la méthode
## Sélection de variable

Le critère LASSO consiste à résoudre le problème d'optimisation suivant : 
$$\widetilde{\mathcal{B}}(\lambda) 
= \arg\min_{\mathcal{B}} \{ \| \widetilde{\mathcal{Y}} - 
\mathcal{\widetilde{X}B} \|^2_2 + \lambda \| \mathcal{B} \|_1  \}$$

### Choix du $\lambda$

Pour mener à bien la procédure il faut choisir un $\lambda$. Le choix fait dans 
`MultiVarSel` consiste à réaliser une validation croisée et à choisir le 
$\lambda_{CV}$.

---

### La *stability selection* 

La méthode (proposée par @meinshausenStabilitySelection2010) consiste à 
échantillonner $nq/2$ indices de $\mathcal{Y}$ et à résoudre le problème 
d'optimisation un grand nombre de fois en relevant les indices des coefficients 
non nuls de $\mathcal{B}$.  
Une fois cela fait, on obtient une fréquence de sélection pour chacun des 
coefficients.

---

Voici un graphique des fréquences obtenues par ordre décroissant en appliquant 
la *stability selection*\footnote{Nous avons fait 5000 réplicats en utilisant 
le \emph{cluster} Migale.}
```{r selection_variables, echo = FALSE, fig.height=2, fig.width=5, fig.cap="Fréquence de sélection par la *stability selection*"}
nb_repli <- 5000
nb.cores <- parallel::detectCores() - 1

filename <- paste0("Freqs_metabolomeFH_TOEPLITZ_nb_repli_", nb_repli, ".Rdata")

if (!file.exists(filename)) {
  Freqs = variable_selection(Y, X, square_root_inv_hat_Sigma, nb_repli = 5000, parallel = TRUE, nb.cores = nb.cores)
  save(Freqs, file = filename)
} else {
  load(filename)
}

colnames(Freqs) <- c("Names_of_Y", "Names_of_X", "frequency")

df_freq_decreasing <- data.frame(
  rank = seq(1, length(sort(Freqs$frequency, decreasing = TRUE))),
  freq = sort(Freqs$frequency, decreasing = TRUE)
)

ggplot(df_freq_decreasing) +
  aes(y = freq, x = rank) +
  ylab("Fréquence") +
  xlab("Rang de l'ordre décroissant") +
  geom_line()
# sort(Freqs$frequency, decreasing = TRUE)[1:50]

seuil <- 0.96
```

---

Sur le graphique, on observe une cassure de la fréquence aux alentours de la 
750e fréquence par ordre décroissant.

Afin de pouvoir interpréter nos résultats plus facilement, notamment du point 
de vue biologique nous allons nous limiter à un seuil de `r seuil`.



## Ré-estimation des paramètres

### Pourquoi ré-estimer ?
Dans le cours (@levy-leducNotesPourCours2024), nous avons vu que les 
Théorèmes 1 et 2 garantissent la consistance en signe des estimateurs des 
$\tilde{\mathcal{B}}$. 

Cependant, l'estimation de la valeur tend à être biaisée,
cette étape nous permet donc de ré-estimer les valeurs des $\mathcal{B}$ qui ont
été estimés non nuls.

---

```{r reestimation, echo = FALSE}

Freqs$Names_of_X <- gsub(pattern = "temperature", replacement = "", Freqs$Names_of_X)
Freqs$Names_of_X <- gsub(pattern = "imbibition", replacement = "", Freqs$Names_of_X)

Freqs <- Freqs %>%
  separate(Names_of_X, into = c("Temperature", "Imbibition"), sep = ":", remove = FALSE)

Freqs$Temperature <- factor(Freqs$Temperature,
  levels = c("Low", "Medium", "Elevated")
)

Freqs$Imbibition <- factor(Freqs$Imbibition,
  levels = c("LI", "EI", "DS")
)

indices <- which(Freqs$frequency >= seuil)
Yvec <- as.numeric(Y %*% square_root_inv_hat_Sigma)
Xvec <- kronecker(t(square_root_inv_hat_Sigma), X)
Xvec_sel <- Xvec[, indices]

# Ré-estimation des B_selectionnés
B_sel_hat <- solve(t(Xvec_sel) %*% Xvec_sel, t(Xvec_sel) %*% Yvec)
Freqs$estim <- rep(0, p * q)
Freqs$estim[indices] <- as.vector(B_sel_hat)
```

```{r graphique, echo = FALSE, fig.height=5.5, fig.cap = "Graphique en boulier des estimations pour les métabolites sélectionnés en fonction de la température et de l'imbibition"}
gr <- ggplot(
  data = Freqs[Freqs$frequency >= seuil, ],
  aes(x = Names_of_Y, y = Temperature, color = estim)
) +
  scale_color_gradient2(low = "steelblue", mid = "white", high = "red") +
  geom_point(size = 2) +
  theme_bw() +
  ylab("") +
  xlab("Métabolites") +
  theme(axis.text.x = element_text(angle = 90)) +
  facet_wrap(~Imbibition, ncol = 1)
gr
```


# Bibliographie {.allowframebreaks}

<div id="refs"></div>

# Merci pour votre attention